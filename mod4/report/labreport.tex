\documentclass[11pt]{article}

\usepackage{graphicx}
\usepackage{framed}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{booktabs}
\usepackage{amsmath}

\marginparwidth 0.5in 
\oddsidemargin 0.25in 
\evensidemargin 0.25in 
\marginparsep 0.25in
\topmargin 0.25in 
\textwidth=6in
\textheight=8in

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolor}{rgb}{0.95,0.95,0.95}

\lstset{
  backgroundcolor=\color{backcolor}, % Set background color
  commentstyle=\color{codegreen}, % Style for comments
  keywordstyle=\color{magenta}, % Style for keywords
  numberstyle=\tiny\color{codegray}, % Style for line numbers
  stringstyle=\color{codepurple}, % Style for strings
  basicstyle=\ttfamily\footnotesize, % Basic font style and size
  breakatwhitespace=false, % Don't break lines at whitespace only
  breaklines=true, % Enable line breaking
  captionpos=b, % Caption position (bottom)
  keepspaces=true, % Keep spaces
  numbers=left, % Show line numbers on the left
  numbersep=5pt, % Separation of numbers from code
  showspaces=false, % Don't show spaces as visible characters
  showstringspaces=false, % Don't show spaces in strings
  showtabs=false, % Don't show tabs as visible characters
  tabsize=2, % Tab size
  language=Python % Specify the language
}


\begin{document}
\hfill\vbox{\hbox{Jude Shin, Torrey Zachs}
		\hbox{CSC 321, Section 07}	
		\hbox{Module 4: Hashing}	
		\hbox{\today}}\par

\bigskip
\centerline{\Large\bf Lab 04: Cryptographic Hash Functions}\par
\bigskip

This lab explores the properties of cryptographic hash functions and find collisions on a truncated range. We explore the Pseudo-randomness and Collision Resistance, and breaking real hashes.

All the code can be found in our remote \href{https://github.com/jude-shin/CSC\_321}{GitHub} repository.

% ============================================================================
% ============================================================================
\section*{Task 1: Exploring Pseudo-Randomness and Collision Resistance}
\subsection*{Part A}
\subsubsection*{Abstract}
This simple function was used to set up the later steps in this task. The function takes in arbitrary inputs and prints the resulting digest to the screen in hexadecimal format.

\subsubsection*{Code}
\begin{lstlisting}
# Hashes an arbitrary input and prints the digest to the screen in hexadecimal
# format.
def part_a(input: bytes, hash_obj: SHA256.SHA256Hash, verbose: bool = True) -> bytes:
    hash_obj.update(input)
    digest: bytes = hash_obj.digest()
    if verbose: 
        print(f'[task1] digest: {digest.hex()}') 
    
    return digest
\end{lstlisting}

\subsection*{Part B}
\subsubsection*{Abstract}
This function takes an arbitrary string, and returns both the original message in bytes form, as well as a randomly augmented value who's bytes were exactly one bit different from the original message. This random bit was chosen in the \verb|get_single_hamm| function. The difference in bytes is called the Hamming Distance. Changing this string would make the newly generated bytes string have a hamming distance of 1 bit. 

\subsubsection*{Code}
\begin{lstlisting}
# Takes a string, turns it into bytes, gets a random string that has a hamming
# distance of exactly 1 bit, and returns the SHA256 values of both of those
def part_b(original: str, hash_obj: SHA256.SHA256Hash) -> tuple[bytes, bytes]:

    # Turn the string of interest into bytes
    original_bytes: bytes = original.encode('utf-8')

    # Get a random string whos hamming distance is exactly 1 bit
    hammed: bytes = get_single_hamm(original_bytes)
    
    hashed_original: bytes = part_a(original_bytes, hash_obj)
    hammed_original: bytes = part_a(hammed, hash_obj)

    return (hashed_original, hammed_original)


# Randomly flips one bit of an arbitrary length byte string, 
# resulting in the hamming distance of 1.
def get_single_hamm(input: bytes) -> bytes:
    byte_array: bytearray = bytearray(input)
    bit_count: int = len(byte_array) * 8

    bit_index: int = random.randrange(bit_count)
    byte_index: int = bit_index//8
    inner_bit_index: int = bit_index%8

    byte_array[byte_index] ^= (1 << inner_bit_index)
    return(bytes(byte_array))
\end{lstlisting}

\subsection*{Part C}
\subsubsection*{Abstract}
The goal of this part of task one was to see if we could find collisions in this hash function. The digests produced by SHA256 were truncated to certain lengths, and based on that, we tried to find two messages that produced the same truncated digest. 

We initially tried the brute force method, where we chose a fixed message\_0, and tried guessing (at random) a message\_1 that would produce the same truncated digest. This would have a max time of $2^{(n)}+1$ where $n$ is the number of bits in the truncated digest. This took too long. 

Instead of waiting, we took advantage of the birthday problem, showing that statistically, giving us a higher chance of finding a collision. Instead of trying to match two messages, we kept track of the digests and the messages that were associated with them as we randomly generated messages of a certain length. Every time we created a new one, we would look it up in the dictionary to see if the digest has been seen before. If it has, (and the original messages were not identical), then we found a collision! This drastically reduced the computation time to somewhere in the order of $2^{(n/2)}$.

We tested these methods with different digest truncation sizes: every even bit count between 8 and 50. The results were recorded in terms of the digest size and the time it took to find a collision, or the time it took and the number of inputs it tried before it found a collision for that digest.

\begin{figure}[!ht]
	\centering
	\begin{subfigure}{1.00\textwidth}
		\centering
		\includegraphics[width=0.5\textwidth]{./assets/digest_sizes_v_collision_times.png}
		\caption{Digest Size vs Time}
		\label{fig:collision_times}
	\end{subfigure}
	\begin{subfigure}{1.00\textwidth}
		\centering
		\includegraphics[width=0.5\textwidth]{./assets/digest_sizes_v_imput_count.png}
		\caption{Digest Size vs Input}
		\label{fig:collision_inputs}
	\end{subfigure}
	\caption{Collision Resistance}
	\label{fig:collision_performance}
\end{figure}

\subsubsection*{Code}
\begin{lstlisting}
# Truncates a digest to a particular
# to a particular domain (in bits)
def part_c(digest: bytes, trunc_len: int) -> bytes:
    # Bytes needed to remove from the digest 
    # working with bytearrays are easier
    truncate_bytes_count: int = trunc_len // 8
    # Remaining fine tuning bits to "remove"
    truncate_bits_count: int = trunc_len % 8

    # Take the first n bytes
    truncated_digest: bytearray = bytearray(digest[:truncate_bytes_count])

    # Bitshift the remaining bits from the digest and tack it on the end of the
    # truncated_digest
    if truncate_bits_count > 0:
        # Note that the remainder of the 'byte' is going to be just zeros
        mask: float = (0xFF << (8-truncate_bits_count)) & 0xFF
        last_byte: float = digest[truncate_bytes_count] & mask
        truncated_digest.append(last_byte)

    return bytes(truncated_digest)

# Takes advantage of the birthday problem 
def process_graphs(hash_obj: SHA256.SHA256Hash) -> None:
    asset_path: str = './assets' 
    dvc_path: str = os.path.join(asset_path, 'digest_sizes_v_collision_times.png')
    dvi_path: str = os.path.join(asset_path, 'digest_sizes_v_imput_count.png') 

    digest_sizes: list[int] = list(range(8, 51, 2))
    collision_times: list[float] = [] # in seconds
    input_count: list[int] = [] # in seconds

    # {truncated digest: message}
    seen: dict[bytes, bytes] = {}

    # =======================================================================

    for b in digest_sizes:
        start_time: float = time.perf_counter()
        j: int = 0

        while True:
            j += 1
           
            # changing the length of this inital input vector did not do that
            # much to help with the time
            m0_bytes: bytes = os.urandom(10)
            m0_digest: bytes = part_a(m0_bytes, hash_obj, False)
            m0_truncated_digest: bytes = part_c(m0_digest, b)
            
            if m0_truncated_digest in seen and m0_bytes != seen[m0_truncated_digest]:
                print(f'[{b}]Collision found at iteration {j} with digest: {m0_truncated_digest}')
                print(f'[{b}]message_0: {m0_bytes}')
                print(f'[{b}]message_1: {seen[m0_truncated_digest]}')
                print('-------------------------\n')

                elapsed_time: float = time.perf_counter() - start_time

                collision_times.append(elapsed_time)
                input_count.append(j)

                break # break the while loop
            else:
                # add it to the list of seen digests
                seen.update({m0_truncated_digest: m0_bytes})


    print(f'digest_sizes: {digest_sizes}')
    print(f'collision_times: {collision_times}')
    print(f'input_count: {input_count}')

    plt.plot(digest_sizes, collision_times)
    plt.xlabel('Digest Sizes (bits)')
    plt.ylabel('Collision Times(seconds)')
    plt.title('Digest Sizes v Collision Times')

    plt.savefig(dvc_path, bbox_inches='tight')
    # plt.show()

    plt.clf()

    plt.plot(digest_sizes, input_count)
    plt.xlabel('Digest Sizes (bits)')
    plt.ylabel('Input (int)')
    plt.title('Digest Sizes v Input Count')

    plt.savefig(dvi_path, bbox_inches='tight')
    # plt.show()

    plt.clf()
\end{lstlisting}

\subsection*{Main Code}
\begin{lstlisting}
if __name__ == '__main__':
    hash_obj: SHA256.SHA256Hash = SHA256.new()

    # Part A
    print('\n--- Task1 Part A ---\n')

    foo_a: bytes = 'Hello, World!'.encode('utf-8')
    part_a(foo_a, hash_obj)

    # Part B 
    print('\n--- Task1 Part B ---\n')

    str0: str = 'Hello'
    str1: str = 'beautiful'
    str2: str = 'world!'

    hashed_str0_original, hashed_str0_hammed = part_b(str0, hash_obj)
    print(f'Original Digest: {hashed_str0_original}')
    print(f'Hammed Digest:   {hashed_str0_hammed}\n')

    hashed_str1_original, hashed_str1_hammed = part_b(str1, hash_obj)
    print(f'Original Digest: {hashed_str1_original}')
    print(f'Hammed Digest:   {hashed_str1_hammed}\n')

    hashed_str2_original, hashed_str2_hammed = part_b(str2, hash_obj)
    print(f'Original Digest: {hashed_str2_original}')
    print(f'Hammed Digest:   {hashed_str2_hammed}\n')

    print('\n--- Task1 Part C ---\n')
    # Part C (option 1 because I am lazy...)
    process_graphs(hash_obj)
\end{lstlisting}

\section*{Environment}
If you want to run and test the code, a virtual environment should first be set up with the correct requirements. This ensures that there is consistency between all of the packages used within this project.

\begin{itemize}
	\item Make a virtual environment (venv) with Python.
		\verb|$ python3 -m venv .venv|
	\item Activate the venv.
		\verb|$ source .venv/bin/activate|
	\item Install the requirements using pip.
		\verb|$ pip install -r requirements.txt|
	\item Whenever you are done, you can deactivate the venv.
		\verb|$ deactivate|
\end{itemize}

\subsection*{Reproduction}
Running any of the code is as simple as activating the venv and then running the python script.

\verb|(.venv) python task3.py|

% ============================================================================
\section*{Task 2: Breaking Real Hashes}
\subsection*{Abstract}
I used a python script to download the nltk\_corpus as obtained from python nltk.corpus.words words.word(). I expect any passwords to be words from this list between 6 and 10 characters. The words are stored in nltk\_corpus.txt, which is then opened by the cpp program. I then use bycrypt to check which of these passwords will result in the correct hash as stored in the shadow.txt file. The cpp program uses the crypt\_r from glibc, a standard linux library. This likely wont work if you are using something else, or don't have glibc. The program spilts the password cracking into 8 threads, which each compute a near equal chunk of the hashes from the corpus.

\subsection*{Code}
The below python program was used to download the nltk\_corpus.txt file.

\begin{lstlisting}
import os
import shutil
from pathlib import Path

try:
    import certifi  #For some reason, I didn't have a valid cert without this to retrieve from nltk
    os.environ.setdefault("SSL_CERT_FILE", certifi.where())
except Exception:
    pass

import nltk

# create a temporary dir in script dir to download nltk_data.
LOCAL_NLTK_DIR = Path.cwd() / "nltk_data"
os.makedirs(LOCAL_NLTK_DIR, exist_ok=True)
if str(LOCAL_NLTK_DIR) not in nltk.data.path:
    nltk.data.path.insert(0, str(LOCAL_NLTK_DIR))

# --- Ensure corpus is present (download to LOCAL_NLTK_DIR if missing) ---
from nltk.corpus import words
try:
    word_list = words.words()
except LookupError:
    nltk.download("words", download_dir=str(LOCAL_NLTK_DIR), quiet=True, raise_on_error=True)
    word_list = words.words()

# --- Filter words 6-10 letters long (inclusive) ---
filtered_words = [w for w in word_list if 6 <= len(w) <= 10]

# Write 6-10 letter words to file in script dir.
output_file = Path.cwd() / "nltk_corpus.txt"
with open(output_file, "w", encoding="utf-8") as f:
    f.write("\n".join(filtered_words))

print(f"Saved {len(filtered_words)} words (6-10 letters) to {output_file}")

# Remove nltk_corpus data directy
if LOCAL_NLTK_DIR.exists():
    shutil.rmtree(LOCAL_NLTK_DIR)
    print(f"Deleted local NLTK data directory: {LOCAL_NLTK_DIR}")
else:
    print("No local NLTK data directory found to delete.")

\end{lstlisting}

Below is the makefile for executing the bcryptCompute cpp program, which expects you to have gcc, but you may be able to change it if you prefer something else. To use the makefile, use "make" and then "make run".
\begin{lstlisting}
CXX      := g++
CXXFLAGS := -std=c++17 -O3 -Wall -Wextra -pthread
LDLIBS   := -lcrypt

TARGET   := bcryptCompute
SRC      := bcryptCompute.cpp

all: $(TARGET)

$(TARGET): $(SRC)
	$(CXX) $(CXXFLAGS) -o $@ $^ $(LDLIBS)

run: $(TARGET)
	./$(TARGET)

clean:
	rm -f $(TARGET)

.PHONY: all clean

\end{lstlisting}

This is the program for computing the passwords. It expects the words to be placed in nltk\_corpus.txt to be line seperated. The python code above genereates this file. The code creates 8 threads that compute b\_crypt for each entry in the shadow.txt file. The threads each compute for a given chunk of the nltk\_corpus.txt file.

\begin{lstlisting}
#include <iostream>
#include <fstream>
#include <sstream>
#include <string>
#include <string_view>
#include <vector>
#include <thread>
#include <atomic>
#include <mutex>
#include <crypt.h>      // crypt_r
#include <sys/stat.h>   // stat
#include <chrono>

#define THREAD_COUNT 8

// globals
std::atomic<bool> stopRequested(false);
std::string foundPassword;
std::mutex foundPasswordMutex;

// Entry type
typedef struct Entry {
    std::string user;
    std::string bcryptInfo; // full stored bcrypt string, e.g. "$2b$08$<salt><hash>"
} Entry;

static long long file_size_bytes(const std::string& path) {
    struct stat st{};
    if (stat(path.c_str(), &st) != 0) return -1;
    return static_cast<long long>(st.st_size);
}

// FORMAT: seconds first, then minutes, then hours.
// Examples: "5 seconds."  "5 seconds. 10 minutes."  "5 seconds. 10 minutes. 1 hours."
static std::string format_elapsed(double total_seconds) {
    long long secs = static_cast<long long>(total_seconds + 0.5); // round seconds
    long long hrs = secs / 3600;
    secs %= 3600;
    long long mins = secs / 60;
    secs %= 60;

    std::ostringstream ss;
    ss << secs << " seconds.";
    if (mins > 0) ss << " " << mins << " minutes.";
    if (hrs > 0) ss << " " << hrs << " hours.";
    return ss.str();
}

// Terminate helper: stores optional pwd, prints msg once, and requests stop.
static void terminate_with_message(const std::string &msg, std::string_view pwd = {}) {
    std::lock_guard<std::mutex> lk(foundPasswordMutex);
    if (stopRequested.load(std::memory_order_acquire)) return;
    if (!pwd.empty()) foundPassword = std::string(pwd);
    std::cout << msg << std::endl;
    stopRequested.store(true, std::memory_order_release);
}

// Function for a thread to compute b_crypt hashes refering to wordlistPath,
// starting at start, end at end, entry contains the hash info to crack
static void worker_chunk(const std::string& wordlistPath,
                        std::streampos start, std::streampos end, Entry entry)
{
    std::ifstream wordList(wordlistPath, std::ios::binary);
    if (!wordList) {
        terminate_with_message("failed to open wordList");
        return;
    }

    wordList.seekg(start);
    if (!wordList) {
        terminate_with_message("failed to seek wordList");
        return;
    }

    if (start > 0) {
        std::string discard;
        std::getline(wordList, discard); // skip partial line
    }

    std::string candidate;
    struct crypt_data cd{};
    cd.initialized = 0;

    while (!stopRequested.load(std::memory_order_acquire) && std::getline(wordList, candidate)) {
        std::streampos pos = wordList.tellg();
        if (pos == std::streampos(-1)) break;
        if (pos > end) break;
        if (candidate.empty()) continue;

        char* out = crypt_r(candidate.c_str(), entry.bcryptInfo.c_str(), &cd);
        if (!out) {
            terminate_with_message(std::string("crypt_r error for user '") + entry.user + "'");
            break;
        }

        if (std::string(out) == entry.bcryptInfo) {
            terminate_with_message(std::string("Found for ") + entry.user + ": " + candidate, std::string_view(candidate));
            break;
        }
    }
}

int main() {
    const std::string shadowFile  = "shadow.txt";
    const std::string wordlist    = "nltk_corpus.txt";
    const std::string outputFile  = "passwords.txt";
    const int startLineNum = 3;

    // parse shadow file, containing the hashes+salt of users whose pw we want to crack
    std::ifstream in(shadowFile);
    if (!in) {
        std::cerr << "Error: could not open " << shadowFile << "\n";
        return 1;
    }

    //store info from the input shadow file into entries
    std::vector<Entry> entries;
    std::string line;
    int curLine = 0;
    while (std::getline(in, line)) {
        if (curLine++ < startLineNum) continue;
        if (line.empty()) continue;
        std::istringstream ss(line);
        Entry e;
        std::getline(ss, e.user, ':');
        std::getline(ss, e.bcryptInfo);
        if (e.bcryptInfo.empty()) {
            std::cerr << "Skipping bad line " << (curLine - 1) << "\n";
            continue;
        }
        entries.push_back(e);
    }
    if (entries.empty()) {
        std::cout << "No entries to process.\n";
        return 0;
    }

    std::ofstream out(outputFile, std::ios::app);
    if (!out) {
        std::cerr << "Error opening output file\n";
        return 1;
    }

    long long totalBytes = file_size_bytes(wordlist);
    if (totalBytes <= 0) {
        std::cerr << "Could not stat wordlist\n";
        return 1;
    }

    for (const Entry& ent : entries) {  //for each pw
        stopRequested.store(false, std::memory_order_release);
        {
            std::lock_guard<std::mutex> lk(foundPasswordMutex);
            foundPassword.clear();
        }

        std::thread workers[THREAD_COUNT];

        //start timer to track how long it take
        auto t0 = std::chrono::steady_clock::now();

        //start the workers
        for (int i = 0; i < THREAD_COUNT; ++i) {
            long long s = (totalBytes * i) / THREAD_COUNT;
            long long epos = (totalBytes * (i + 1)) / THREAD_COUNT; //record start time for this pw
            workers[i] = std::thread(worker_chunk,
                                     wordlist,
                                     static_cast<std::streampos>(s),
                                     static_cast<std::streampos>(epos),
                                     ent); 
        }
        //join workers
        for (int i = 0; i < THREAD_COUNT; ++i) {
            if (workers[i].joinable()) workers[i].join();
        }

        //record how long it took
        auto t1 = std::chrono::steady_clock::now();
        std::chrono::duration<double> elapsed = t1 - t0;
        std::string elapsed_str = format_elapsed(elapsed.count());

        //print results
        {
            std::lock_guard<std::mutex> lk(foundPasswordMutex);
            if (!foundPassword.empty()) {
                std::cout << "Found for " << ent.user
                          << " (time=" << elapsed_str << "): " << foundPassword << "\n";
                out << ent.user << " " << foundPassword << " " << elapsed_str << "\n";
                out.flush();
            } else {
                std::cout << "Not found for " << ent.user
                          << " (time=" << elapsed_str << ")\n";
            }
        }
    }

    return 0;
}
\end{lstlisting}

% ============================================================================
\subsection*{Results}
Bilbo welcome 39 seconds. 2 minutes.\\
Gandalf wizard 27 seconds. 3 minutes.\\
Thorin diamond 24 seconds. 3 minutes.\\
Fili desire 28 seconds. 6 minutes.\\
Kili ossify 13 seconds. 4 minutes.\\
Balin hangout 41 seconds. 14 minutes.\\
Dwalin drossy 36 seconds.\\
Oin ispaghul 15 seconds. 6 minutes.\\
Gloin oversave 56 seconds. 19 minutes.\\
Dori indoxylic 25 seconds. 8 minutes.\\
Nori swagsman 37 seconds. 18 minutes.\\
Ori airway 54 seconds. 8 minutes.\\
Bifur corrosible 29 seconds. 30 minutes.\\
Bofur libellate 14 seconds. 42 minutes.\\
Durin purrone 52 seconds. 45 minutes.\\



\section*{Questions}
\subsection*{Question 1}
Even though the original strings were offset by a difference of only one bit, it still drastically changed the digest. This is pretty reassuring; the output is not predictable, and it makes a very big difference.

\subsection*{Question 2}
For a hash with a fixed output size $(n)$, by definition, the digest is $256$ bits, where a bit can be either a $0$ or a $1$ (there are two options for every bit). This results in a maximum space of $2^{n}$ different possibilities. Therefore, if you somehow managed to get $2^{n}$ unique hashes, from $2^{n}$ arbitrary inputs, then the next hash you take must map to a digest in the vector-space that has already been visited. So the maximum number of hashes needed to encounter a collision would have to be $2^{n}$+1 (this is assuming you have hashed one fixed message, and try all other $2^{n}$ combinations on a different input message). However, this is not what happens all of the time. The Birthday Paradox shows us that the expected number of files to guess in order to find a collision is $2^{(n/2)}$.

Given the data that we collected (on Jude's laptop), the average time it took to process a single input was $4.206261571130722 \times 10^{-6}$ seconds. If we stuck with the brute force method, we would calculate $(2^{256}) \cdot (4.206261571130722 \times 10^{-6})$, which comes out to $4.889624740283841 \times 10^{71}$ seconds, or $1.5495 \times 10^{64}$ years. Even if we try to leverage the birthday problem, the average time would be $(2^{256/2}) \cdot (4.206261571130722 \times 10^{-6})$, which comes out to $1.4313166 \times 10^{33}$ seconds, or $4.5357 \times 10^{25}$ years. Both are insanely incomprehensible.

\subsection*{Question 3}
There are only $O(2^{n})$ possibilities; if the digest size is $8$ bits, then there are only $256$ different possibilities to brute force through. Because of the birthday problem, the number of attempts to find a collision reduces down to $2^{(8/2)}$, which comes out to be $16$ tries (significantly less). To be honest, both cases are relatively trivial to break.

The mapping from input to digest is one way, but inherritaly, there are going to be collisions because no hash is perfect. Because this is the case, there is a one to many relationship when trying to use a digest to get an input (or preimage). In this case, we are analyzing trying to find only the first preimage.

\subsection*{Question 4}
For a password that contains one additional word, given that we are comparing with
\(130{,}000\) words, I would expect it to take approximately \(130{,}000\) times longer,
since we are going from \(O(n)\) on number of words to \(O(n^{2})\).
Likewise, if there were three words, I would expect it to take \(130{,}000^{2}\) times
longer, since we go from \(O(n)\) to \(O(n^{3})\).

Given that for one word, a work factor of 8 took about 3 minutes, and a work factor of 12
took about 40 minutes on average, and that the conversion from minutes to years is

\begin{table}[h!]
    \centering
    \caption{Expected Password Cracking Time in Years}
    \label{tab:results}
    \begin{tabular}{l c c}
        \toprule
        \textbf{Work Factor} & \textbf{2 Words ($\mathbf{\times N}$)} & \textbf{3 Words ($\mathbf{\times N^2}$)} \\
        \midrule
        \textbf{WF = 8} & $ \dfrac{130{,}000 \cdot 3}{525{,}600} \approx 0.742\ \mathrm{yr} $ & $ \dfrac{130{,}000^{2} \cdot 3}{525{,}600} \approx 96{,}461\ \mathrm{yr} $ \\
        \textbf{WF = 12} & $ \dfrac{130{,}000 \cdot 40}{525{,}600} \approx 9.89\ \mathrm{yr} $ & $ \dfrac{130{,}000^{2} \cdot 40}{525{,}600} \approx 1{,}286{,}149\ \mathrm{yr} $ \\
        \bottomrule
    \end{tabular}
\end{table}

\end{document}
